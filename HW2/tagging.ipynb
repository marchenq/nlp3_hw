{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импорт необходимых модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import spacy\n",
    "import pymorphy2\n",
    "import stanza as stanza\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подключение к базе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('pos.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Получение текста и золотого стадарта из базы данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тексте достаточно много слов, с которыми у программы могут возникнуть сложности: преимущественно это морфологические омонимы (\"пила\": VERB|NOUN, \"простой\": ADJ|NOUN, \"дуло\": VERB|NOUN и т.д.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Год назад я не пила кофе после обеда. Иначе простой или перегон пустых вагонов станут обычным делом. Просто строчка, просто стих, который требовал продолжения, пытаясь всосать в себя пространство, словно удав, чтобы генерировать это самое продолжение. В два часа пополудни ветер внезапно стих, но туча уже закрыла окрестные горы и начала заслонять солнце. Моя посуду, всё думала, почему Мише не повезло, почему он не состоялся как писатель. Мы уже полдня полем сорняки и всё никак не можем закончить. Стволы спиленных деревьев и выкорчеванные мощные пни было решено не уничтожать, а использовать при создании декоративных ландшафтных композиций. Даже не властвуя, женщина Тургенева всегда смела или, по крайней мере, сильна: такова Лиза, такова Елена. Данзас подал Пушкину другой пистолет: дуло первого при падении забилось снегом. Сегодня река голубей, чем была вчера. В казарме на всех стенах висели мечи. За медведем погнался рой пчел, чтобы проучить его. Я бы, наверное, и вправду влепил Косте оплеуху, если бы в чаще травы в эту минуту не мелькнула спина рыжего мирмика. С острым, точно лезвие, до дрожи сердечным чувством ― поля, простор и русский лес, и родные дали с затерянными в них редкими огоньками. Показано, что вдоль границы раздела возможно распространение электромагнитной волны, поля которой убывают по экспоненте в обе стороны от плоскости раздела решёток.\n",
      "Manual tagging: ['NOUN', 'ADVB', 'PRON', 'PRLC', 'VERB', 'NOUN', 'PREP', 'NOUN', 'ADVB', 'NOUN', 'CONJ', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'PRCL', 'NOUN', 'PRCL', 'NOUN', 'ADJ', 'VERB', 'NOUN', 'GRND', 'VERB', 'PREP', 'PRON', 'NOUN', 'CONJ', 'NOUN', 'CONJ', 'VERB', 'PRCL', 'ADJ', 'NOUN', 'PREP', 'NUM', 'NOUN', 'ADVB', 'NOUN', 'ADVB', 'VERB', 'CONJ', 'NOUN', 'ADVB', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'VERB', 'VERB', 'NOUN', 'GRND', 'NOUN', 'PRCL', 'VERB', 'ADVB', 'NOUN', 'PRCL', 'VERB', 'ADVB', 'PRON', 'PRCL', 'VERB', 'CONJ', 'NOUN', 'PRON', 'ADVB', 'NOUN', 'VERB', 'NOUN', 'CONJ', 'ADJ', 'PRCL', 'PRCL', 'VERB', 'VERB', 'NOUN', 'PRT', 'NOUN', 'CONJ', 'PRT', 'ADJ', 'NOUN', 'VERB', 'PRT', 'PRCL', 'VERB', 'CONJ', 'VERB', 'PREP', 'NOUN', 'ADJ', 'ADJ', 'NOUN', 'PRCL', 'PRCL', 'GRND', 'NOUN', 'NOUN', 'ADVB', 'VERB', 'CONJ', 'PREP', 'ADJ', 'NOUN', 'ADJ', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'PREP', 'NOUN', 'VERB', 'NOUN', 'ADVB', 'NOUN', 'ADJ', 'CONJ', 'VERB', 'ADVB', 'PREP', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PREP', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'VERB', 'PRON', 'PRON', 'PRCL', 'ADVB', 'CONJ', 'ADVB', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'PRCL', 'PREP', 'NOUN', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'PRCL', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'PREP', 'ADJ', 'AVB', 'NOUN', 'PREP', 'NOUN', 'ADJ', 'NOUN', '', 'NOUN', 'NOUN', 'CONJ', 'ADJ', 'NOUN', 'CONJ', 'ADJ', 'NOUN', 'PREP', 'ADJ', 'PREP', 'PRON', 'ADJ', 'NOUN', 'PRED', 'CONJ', 'PREP', 'NOUN', 'NOUN', 'PRED', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'VERB', 'PREP', 'NOUN', 'PREP', 'NUM', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.replace(',', '')\n",
    "    text = text.replace(':', '')\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace('?', '')\n",
    "    return text\n",
    "\n",
    "cur.execute('SELECT sent FROM corpus')\n",
    "testText = ' '.join([text[0] for text in cur.fetchall()])\n",
    "text = clean(testText)\n",
    "print('Text:', testText)\n",
    "\n",
    "cur.execute('SELECT tags FROM corpus')\n",
    "golden_standard = ','.join([text[0] for text in cur.fetchall()])\n",
    "golden_standard = golden_standard.split(',')\n",
    "print('Manual tagging:', golden_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стандартизация разметки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку не все из исследуемых теггеров выделяют краткие формы прилагательных и причастий, а также их сравнительные степени, объединим их одним тегом. Аналогично не все теггеры выделяют инфинитивы глаголов, так что объединим их одним тегом. Остальные теги просто приводятся к общему: например, частицы в pyMorphy обозначаются как PRCL, а в Spacy и Stanza как PART - сделаем их все PRCL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DEFAULT TAG SET\n",
    "ADJ, ADVB, NOUN, PRON, PREP, INFN, CONJ, PRCL, GRND, NUM, PRT\n",
    "'''\n",
    "\n",
    "def standartize(lst):\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == 'ADJF' or lst[i] == 'ADJS' or lst[i] == 'COMP' or lst[i] == 'DET':\n",
    "            lst[i] = 'ADJ'\n",
    "        elif lst[i] == 'ADV':\n",
    "            lst[i] = 'ADVB'\n",
    "        elif lst[i] == 'PART':\n",
    "            lst[i] = 'PRCL'\n",
    "        elif lst[i] == 'SCONJ' or lst[i] == 'CCONJ':\n",
    "            lst[i] = 'CONJ'\n",
    "        elif lst[i] == 'ADP':\n",
    "            lst[i] = 'PREP'\n",
    "        elif lst[i] == 'INFN' or lst[i] == 'AUX':\n",
    "            lst[i] = 'VERB'\n",
    "        elif lst[i] == 'PRTS' or lst[i] == 'PRTF':\n",
    "            lst[i] = 'PRT'\n",
    "        elif lst[i] == 'NPRO':\n",
    "            lst[i] = 'PRON'\n",
    "        elif lst[i] == 'PROPN':\n",
    "            lst[i] = 'NOUN'\n",
    "        elif lst[i] == 'NUMR':\n",
    "            lst[i] = 'NUM'\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разметка pyMorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyMorphy tags: ['NOUN', 'ADVB', 'PRON', 'PRCL', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'ADVB', 'ADJ', 'CONJ', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'PRCL', 'NOUN', 'PRCL', 'NOUN', 'ADJ', 'VERB', 'NOUN', 'GRND', 'VERB', 'PREP', 'PRON', 'NOUN', 'CONJ', 'NOUN', 'CONJ', 'VERB', 'PRCL', 'ADJ', 'NOUN', 'PREP', 'NUM', 'NOUN', 'ADVB', 'NOUN', 'ADVB', 'NOUN', 'CONJ', 'NOUN', 'ADVB', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'PRCL', 'VERB', 'ADVB', 'NOUN', 'PRCL', 'VERB', 'ADVB', 'PRON', 'PRCL', 'VERB', 'CONJ', 'NOUN', 'PRON', 'ADVB', 'NOUN', 'NOUN', 'NOUN', 'CONJ', 'PRCL', 'ADVB', 'PRCL', 'VERB', 'VERB', 'NOUN', 'PRT', 'NOUN', 'CONJ', 'PRT', 'ADJ', 'NOUN', 'VERB', 'PRT', 'PRCL', 'VERB', 'CONJ', 'VERB', 'PREP', 'NOUN', 'ADJ', 'ADJ', 'NOUN', 'PRCL', 'PRCL', 'GRND', 'NOUN', 'NOUN', 'ADVB', 'VERB', 'CONJ', 'PREP', 'ADJ', 'NOUN', 'ADJ', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'PREP', 'NOUN', 'VERB', 'NOUN', 'ADVB', 'NOUN', 'VERB', 'CONJ', 'VERB', 'ADVB', 'PREP', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PREP', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'VERB', 'PRON', 'PRON', 'PRCL', 'CONJ', 'CONJ', 'ADVB', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'PRCL', 'PREP', 'ADJ', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'PRCL', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'PREP', 'ADJ', 'ADVB', 'NOUN', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'None', 'NOUN', 'NOUN', 'CONJ', 'ADJ', 'NOUN', 'CONJ', 'ADJ', 'VERB', 'PREP', 'ADJ', 'PREP', 'PRON', 'ADJ', 'NOUN', 'PRT', 'CONJ', 'PREP', 'NOUN', 'NOUN', 'CONJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'VERB', 'PREP', 'NOUN', 'PREP', 'NUM', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN']\n",
      "PyMorphy accuracy: 0.9139\n"
     ]
    }
   ],
   "source": [
    "def pyMorphyTagging(text):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    pyMorhy_tags = [str(morph.parse(word)[0].tag.POS) for word in text.split(' ')]\n",
    "    return standartize(pyMorhy_tags)\n",
    "\n",
    "pyMorphy_pos = pyMorphyTagging(text)\n",
    "print('PyMorphy tags:', pyMorphy_pos)\n",
    "print('PyMorphy accuracy: %.4f' % accuracy_score(pyMorphy_pos, golden_standard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разметка Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanza tags: ['NOUN', 'ADVB', 'PRON', 'PRCL', 'VERB', 'NOUN', 'PREP', 'NOUN', 'ADVB', 'ADJ', 'CONJ', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'PRCL', 'NOUN', 'PRCL', 'NOUN', 'PRON', 'VERB', 'NOUN', 'VERB', 'VERB', 'PREP', 'PRON', 'NOUN', 'CONJ', 'NOUN', 'CONJ', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'PREP', 'NUM', 'NOUN', 'ADVB', 'NOUN', 'ADVB', 'NOUN', 'CONJ', 'NOUN', 'ADVB', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'VERB', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'PRON', 'VERB', 'ADVB', 'NOUN', 'PRCL', 'VERB', 'ADVB', 'PRON', 'PRCL', 'VERB', 'CONJ', 'NOUN', 'PRON', 'ADVB', 'ADVB', 'NOUN', 'NOUN', 'CONJ', 'PRON', 'ADVB', 'PRCL', 'VERB', 'VERB', 'NOUN', 'VERB', 'NOUN', 'CONJ', 'VERB', 'ADJ', 'NOUN', 'VERB', 'VERB', 'PRCL', 'VERB', 'CONJ', 'VERB', 'PREP', 'NOUN', 'ADJ', 'ADJ', 'NOUN', 'PRCL', 'PRCL', 'VERB', 'NOUN', 'NOUN', 'ADVB', 'VERB', 'CONJ', 'PREP', 'ADJ', 'NOUN', 'ADJ', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'PREP', 'NOUN', 'VERB', 'NOUN', 'ADVB', 'NOUN', 'NOUN', 'PRON', 'VERB', 'ADVB', 'PREP', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PREP', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'VERB', 'PRON', 'PRON', 'VERB', 'ADVB', 'CONJ', 'ADVB', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'VERB', 'PREP', 'ADVB', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'PRCL', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'PREP', 'ADJ', 'ADVB', 'NOUN', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'CONJ', 'ADJ', 'NOUN', 'CONJ', 'ADJ', 'VERB', 'PREP', 'VERB', 'PREP', 'PRON', 'ADJ', 'NOUN', 'VERB', 'CONJ', 'PREP', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'PREP', 'NUM', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN']\n",
      "Stanza accuracy: 0.8660\n"
     ]
    }
   ],
   "source": [
    "def stanzaTagging(text):\n",
    "    nlp = stanza.Pipeline('ru', processors='tokenize,pos', verbose=False)\n",
    "    doc = nlp(text)\n",
    "    stanza_tags = [str(word.upos) for sentence in doc.sentences for word in sentence.words]\n",
    "    return standartize(stanza_tags)\n",
    "\n",
    "stanza_pos = stanzaTagging(text)\n",
    "print('Stanza tags:', stanza_pos)\n",
    "print('Stanza accuracy: %.4f' % accuracy_score(stanza_pos, golden_standard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разметка Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy tags: ['NOUN', 'ADVB', 'PRON', 'PRCL', 'VERB', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'ADJ', 'CONJ', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'PRCL', 'NOUN', 'PRCL', 'NOUN', 'PRON', 'VERB', 'NOUN', 'VERB', 'VERB', 'PREP', 'PRON', 'NOUN', 'CONJ', 'VERB', 'CONJ', 'VERB', 'PRON', 'ADJ', 'NOUN', 'PREP', 'NUM', 'NOUN', 'NOUN', 'NOUN', 'ADVB', 'NOUN', 'CONJ', 'NOUN', 'ADVB', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'VERB', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'PRON', 'VERB', 'ADVB', 'NOUN', 'PRCL', 'VERB', 'ADVB', 'PRON', 'PRCL', 'VERB', 'CONJ', 'NOUN', 'PRON', 'ADVB', 'ADVB', 'NOUN', 'NOUN', 'CONJ', 'PRON', 'ADVB', 'PRCL', 'VERB', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'CONJ', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'VERB', 'PRCL', 'VERB', 'CONJ', 'VERB', 'PREP', 'NOUN', 'ADJ', 'ADJ', 'NOUN', 'PRCL', 'PRCL', 'VERB', 'NOUN', 'NOUN', 'ADVB', 'VERB', 'CONJ', 'ADVB', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'ADJ', 'PREP', 'NOUN', 'VERB', 'NOUN', 'ADVB', 'NOUN', 'NOUN', 'PRON', 'VERB', 'ADVB', 'PREP', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'PREP', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'VERB', 'PRON', 'PRON', 'VERB', 'ADVB', 'CONJ', 'ADVB', 'VERB', 'NOUN', 'VERB', 'CONJ', 'VERB', 'PREP', 'ADVB', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'PRCL', 'VERB', 'NOUN', 'ADJ', 'NOUN', 'PREP', 'ADJ', 'ADVB', 'NOUN', 'PREP', 'NOUN', 'ADJ', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'CONJ', 'ADJ', 'NOUN', 'CONJ', 'NOUN', 'VERB', 'PREP', 'ADJ', 'PREP', 'PRON', 'ADJ', 'NOUN', 'VERB', 'CONJ', 'PREP', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'PREP', 'NUM', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN']\n",
      "Spacy accuracy: 0.8325\n"
     ]
    }
   ],
   "source": [
    "def spacyTagging(text):\n",
    "    nlp = spacy.load(\"ru_core_news_sm\")\n",
    "    doc = nlp(text)\n",
    "    spacy_tags = [str(word.pos_) for sentence in doc.sents for word in sentence]\n",
    "    return standartize(spacy_tags)\n",
    "\n",
    "spacy_pos = spacyTagging(text)\n",
    "print('Spacy tags:', spacy_pos)\n",
    "print('Spacy accuracy: %.4f' % accuracy_score(spacy_pos, golden_standard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выделение n-грамм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку наиболее точным в разметке из трёх рассмотренных теггеров оказался pyMorphy (91.39%), используем его для выделения n-грамм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "НАР+ГЛ: ['уже закрыла', 'всегда смела', 'вправду влепил']\n",
      "ПРЕДЛ+ПРИЛ+СУЩ: ['по крайней мере', 'на всех стенах', 'в чаще травы', 'в эту минуту']\n",
      "ПРЕДЛ+СУЩ+ГЛ: ['при падении забилось', 'За медведем погнался']\n"
     ]
    }
   ],
   "source": [
    "def chunker(bigram, text, pos):\n",
    "    chunk = [(i, i+len(bigram)) for i in range(len(pos)-len(bigram)+1) if pos[i:i+len(bigram)] == bigram]\n",
    "    res = [' '.join(text.split(' ')[seq[0]:seq[1]]) for seq in chunk]\n",
    "    return res\n",
    "\n",
    "AV_bigram = ['ADVB', 'VERB']\n",
    "PAN_trigram = ['PREP', 'ADJ', 'NOUN']\n",
    "PNVN_trigram = ['PREP', 'NOUN', 'VERB']\n",
    "print('НАР+ГЛ:', chunker(AV_bigram, text, pyMorphy_pos))\n",
    "print('ПРЕДЛ+ПРИЛ+СУЩ:', chunker(PAN_trigram, text, pyMorphy_pos))\n",
    "print('ПРЕДЛ+СУЩ+ГЛ:', chunker(PNVN_trigram, text, pyMorphy_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Улучшение первого ДЗ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для улучшения качества предсказания из первого домашнего задания предлагаю использовать биграмму \"didn't+VERB\", тогда сможем однозначно выделять отрицательные отзывы типа \"didn't enjoy\". В отзывах также часто встречаются оценки вида \"the best movie\", так что может быть полезным выделять триграммы \"the+ADJ(SUPERL)+movie\". Очень часто усилительное наречие so встречается в оценках вида \"so boring\" или \"so awesome\", причём обычно прилагательным при нём подчеркивается основная линия отзыва, так что можно выделять биграммы \"so+ADJ\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams in good reviews only: {'did not experience', 'so pure', 'so believable', 'the best movie', 'so perfect', 'so amazing', 'so creative', 'did not watch', \"didn't get\", \"didn't watch\", 'so fantastic', 'so captivating', 'so talented', \"didn't look\", 'so ravishing', \"didn't need\", 'so awesome', 'so incredible', \"didn't win\", \"didn't cry\", 'so beautiful', 'so excited', 'the greatest movie', 'so emotional', 'so glad', 'so gripping', \"didn't know\", 'so impressed', 'so good', 'so deep', 'so intricate', 'so lit', 'did not see', \"didn't expect\", 'so bittersweet', 'so different', 'so great', 'did not receive', 'so beuautiful', \"didn't destroy\"}\n",
      "N-grams in bad reviews only: {\"didn't like\", 'so awful', 'did not make', \"didn't feel\", 'so boring', 'so hopeful', \"didn't do\", 'so stupid', 'did not understand', 'so great.In', \"didn't use\", 'did not contain', 'so incomprehensible', 'so angry', 'did not mind', 'so smart', \"didn't add\", 'so dramatic', \"didn't make\", 'so predictable', 'so blunt', 'so sorry', 'so secret', 'the worst movie', 'so attractive', 'so loud', 'so immature', 'so disappointing', 'so da', 'so bizarre', 'so tired', 'did not get'}\n",
      "Accuracy: 0.8043\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('reviews.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"SELECT text FROM reviews WHERE polarity='pos'\")\n",
    "goodReviews = [review[0] for review in cur.fetchall()]\n",
    "cur.execute(\"SELECT text FROM reviews WHERE polarity='neg'\")\n",
    "badReviews = [review[0] for review in cur.fetchall()]\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmmatizer=WordNetLemmatizer()\n",
    "\n",
    "def get_pos(reviews):\n",
    "    pos = []\n",
    "    for review in reviews:\n",
    "        text = nltk.tokenize.word_tokenize(review)\n",
    "        pos.append(nltk.pos_tag(text))\n",
    "    return pos\n",
    "\n",
    "good_pos = get_pos(goodReviews)\n",
    "bad_pos = get_pos(badReviews)\n",
    "\n",
    "def get_ngrams(pos):\n",
    "    so_bigrams = []\n",
    "    didnt_bigrams = []\n",
    "    theADJmovie_trigrams = []\n",
    "    for entry in pos:\n",
    "        for i, word in enumerate(entry):\n",
    "            if word[0] == 'so' and entry[i+1][1] == 'JJ':\n",
    "                so_bigrams.append(' '.join((word[0], entry[i+1][0])))\n",
    "            elif word[0] == 'did' and entry[i+1][0] == \"n't\" and entry[i+2][1] == 'VB':\n",
    "                didnt_bigrams.append(' '.join((word[0]+entry[i+1][0], entry[i+2][0])))\n",
    "            elif word[0] == 'did' and entry[i+1][0] == 'not' and entry[i+2][1] == 'VB':\n",
    "                didnt_bigrams.append(' '.join((word[0], entry[i+1][0], entry[i+2][0])))\n",
    "            elif word[0] == 'the' and entry[i+1][1] == 'JJS' and entry[i+2][0] == 'movie':\n",
    "                theADJmovie_trigrams.append(' '.join((word[0], entry[i+1][0], entry[i+2][0])))\n",
    "    return so_bigrams, didnt_bigrams, theADJmovie_trigrams\n",
    "\n",
    "so_bigrams_good, didnt_bigrams_good, theADJmovie_trigrams_good = get_ngrams(good_pos)\n",
    "so_bigrams_bad, didnt_bigrams_bad, theADJmovie_trigrams_bad = get_ngrams(bad_pos)\n",
    "\n",
    "so_bigrams_good_only = set(so_bigrams_good).difference(set(so_bigrams_bad))\n",
    "so_bigrams_bad_only = set(so_bigrams_bad).difference(set(so_bigrams_good))\n",
    "didnt_bigrams_good_only = set(didnt_bigrams_good).difference(set(didnt_bigrams_bad))\n",
    "didnt_bigrams_bad_only = set(didnt_bigrams_bad).difference(set(didnt_bigrams_good))\n",
    "theADJmovie_trigrams_good_only = set(theADJmovie_trigrams_good).difference(set(theADJmovie_trigrams_bad))\n",
    "theADJmovie_trigrams_bad_only = set(theADJmovie_trigrams_bad).difference(set(theADJmovie_trigrams_good))\n",
    "\n",
    "ngrams_good_only = so_bigrams_good_only | didnt_bigrams_good_only | theADJmovie_trigrams_good_only\n",
    "ngrams_bad_only = so_bigrams_bad_only | didnt_bigrams_bad_only | theADJmovie_trigrams_bad_only\n",
    "\n",
    "print('N-grams in good reviews only:', ngrams_good_only)\n",
    "print('N-grams in bad reviews only:', ngrams_bad_only)\n",
    "\n",
    "cur.execute('SELECT text FROM test')\n",
    "testReviews = [review[0] for review in cur.fetchall()]\n",
    "\n",
    "predictedScores = []\n",
    "for review in testReviews:\n",
    "    posTest = []\n",
    "    text = nltk.tokenize.word_tokenize(review)\n",
    "    posTest.append(nltk.pos_tag(text))\n",
    "    so_bigrams_test, didnt_bigrams_test, theADJmovie_trigrams_test = get_ngrams(posTest)\n",
    "    test_ngrams = set(so_bigrams_test) | set(didnt_bigrams_test) | set(theADJmovie_trigrams_test)\n",
    "    if len(test_ngrams.intersection(ngrams_good_only)) >= len(test_ngrams.intersection(ngrams_bad_only)):\n",
    "        predictedScores.append('pos')\n",
    "    else:\n",
    "        predictedScores.append('neg')\n",
    "\n",
    "cur.execute('SELECT polarity FROM test')\n",
    "actualScores = [score[0] for score in cur.fetchall()]\n",
    "\n",
    "print('Accuracy: %.4f' % accuracy_score(predictedScores, actualScores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность предсказания осталась примерно той же: поднялась на 2 процентных пункта. Связано это скорее со слишком малым числом тестовых отзывов (их в целом довольно мало на Марсианина)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
